%========================
% Document class and theme
%========================
\documentclass[8pt]{beamer}
\usetheme[progressbar=frametitle]{metropolis}
\setbeamersize{text margin left=10mm, text margin right=10mm}
\usepackage{appendixnumberbeamer} % appendix slide numbering
\setbeamertemplate{theorems}[numbered]

%========================
% Core packages
%========================
\usepackage{amsmath, amsfonts, amssymb, amsthm} % math + theorems
\usepackage{booktabs}        % professional tables
\usepackage{hyperref}        % hyperlinks
\usepackage{xcolor}          % colors
\usepackage{xspace}          % spacing for custom commands

%========================
% Algorithms
%========================
\usepackage{algorithm}
\usepackage{algpseudocode}
\newtheorem{proposition}{Proposition}
\usepackage{bbm}

%========================
% Plots and TikZ
%========================
\usepackage{pgfplots}
\usepgfplotslibrary{dateplot}
\usepackage{tikz}
\usetikzlibrary{positioning}

% ==========================================
% Professional Code Listing Setup
% ==========================================
\usepackage{listings}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}

%========================
% Custom commands
%========================
\newcommand{\themename}{\textbf{\textsc{metropolis}}\xspace}

%========================
% Custom footline
%========================
\setbeamertemplate{footline}
{%
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.35\paperwidth,ht=2.5ex,dp=1.5ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.5ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.5ex,dp=1ex,right]{date in head/foot}%
    \usebeamerfont{date in head/foot}\insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

%========================
% Beamer tweaks
%========================
\setbeamertemplate{navigation symbols}{} % remove default navigation symbols

\title{Chapter 3 - Monte Carlo Methods}
\subtitle{Monte Carlo Applications to Statistical Inference.\\ Point Estimators.}
\author{Prof. Alex Alvarez, Ali Raisolsadat}
\institute{School of Mathematical and Computational Sciences \\ University of Prince Edward Island}
\date{} % leave empty or add \today

%========================
% Begin document
%========================
\begin{document}

%-------------------
% Title frame
%-------------------
\maketitle

%-------------------
% Slide 1: Statistical Inference Problems
%-------------------
\begin{frame}{Statistical Inference Problems}
The Monte Carlo methods studied in previous lectures may be used to solve some practical Statistical Inference problems.

\vspace{2mm}

Consider for example the classical inference problem of point estimation.

\vspace{2mm}

Suppose we have some observations $x=(x_1,x_2,...,x_n)$ that are considered a random sample of a random variable $X=(X_1,X_2,...,X_n)\sim P_{\theta}$, where $\theta \in \Theta$ is an unknown parameter,
and the problem is to estimate  $\theta$ from the observations $x=(x_1,x_2,...,x_n)$.

\vspace{2mm}

An \textbf{estimator} of $\theta$ is a function $\hat{\theta}=\hat{\theta}(X)=\hat{\theta}(X_1,X_2,...,X_n)$. Notice that $\hat{\theta}(X)$ is a random variable.
\vspace{2mm}

The value of the estimator for the observed data $x$, $\hat{\theta}(x)$ is an {\bf estimate} of $\theta$. Notice that $\hat{\theta}(x)$ is a point in $\Theta$.
\end{frame}

%-------------------
% Slide 2: Quality of an Estimator
%-------------------
\begin{frame}{Quality of an Estimator $\hat{\theta}(X)$}
Good estimators $\hat{\theta}=\hat{\theta}(X)$ of the parameter $\theta$ should have a distribution that concentrates around $\theta$.
Specifically, the bias and the variance of an estimator give a good measure of its quality:
\begin{equation*}
	MSE (\hat{\theta}) = bias^2(\hat{\theta}) + Var(\hat{\theta})
\end{equation*}

There are several cased in which we can find the bias and variance explicitly, for example

\begin{itemize}
	\item If the random variables $X_i$ are i.i.d. and $\theta=E(X_i)$, the empirical mean estimator is unbiased.
	\item If n is large,sometimes we can use some probability tools such as the Central Limit Theorem.
\end{itemize}

In other cases, finding the bias and the variance of an estimator explicitly is not possible. Is in situations like these, that Monte Carlo methods may be useful.
\end{frame}

%-------------------
% Slide 3: Estimating the Bias
%-------------------
\begin{frame}{Estimating the Bias}
We start with
\begin{equation*}
	bias_{\theta}(\hat{\theta})=E(\hat{\theta}(X))-\theta
\end{equation*}

For a given value of $\theta$, if we are able to generate $N$ samples: $\left\{x^{(j)}\right\}_{j=1,2,...,N}$ of the random variable $X$ we could estimate the bias with

\begin{equation*}
	\widehat{bias}_{\theta}\left(\hat{\theta}\right)=\frac{1}{N} \sum_{j=1}^N \hat{\theta}\left(x^{(j)}\right)-\theta
\end{equation*}

We can do this for a range of values of $\theta$ to get an understanding of the bias of an estimator as a function of $\theta$. 
\end{frame}

%-------------------
% Slide 4: Estimating the Bias Remarks
%-------------------
\begin{frame}{Remarks}
\begin{itemize}
	\item Notice that we will generate N samples of the random variable $X$.
	\item The j-th sample will be of the form $x^{(j)}=\left(x_1^{(j)},x_2^{(j)},...,x_n^{(j)}\right)$
	\item In order to estimate the bias of $\hat{\theta}$ for a fixed value $\theta$ we will have to generate $N\times n$ random values.
	\item It is important not to confuse $n$ (the size of the samples in the estimator $\hat{\theta}$) with N (the number of samples used to estimate the bias).
\end{itemize}
\end{frame}

%-------------------
% Slide 5: Example - Estimating the Bias Remarks
%-------------------
\begin{frame}{Example - Estimating the Bias} 
Consider that you are given the following sample of independent random numbers and you are told that they come from a uniform distribution $[0, \theta]$ where the parameter $\theta$ is unknown. The problem is to estimate $\theta$.
\vspace{2mm}

0.90648057    \hspace{1mm} 0.60372528  \hspace{1mm}
1.06419351   \hspace{1mm} 3.38445752  \hspace{1mm}
0.05913391 \hspace{1mm} \\
2.23352471 \hspace{1mm}
2.55686224 \hspace{1mm}  0.20315035 \hspace{1mm}
1.11948303 \hspace{1mm} 2.38614136
\vspace{2mm}
\pause

One approach that we can take in this example is to use the naive estimator $\hat{\theta}(X)=\max(X_1,X_2,...,X_{10})$, so for this specific sample we will have $\hat{\theta}(x)=3.38445752$.
\vspace{2mm}

The naive estimator  $\hat{\theta}(X)$ always underestimates the value of $\theta$. In other words, we know that the estimator is (negatively) biased. We could try to estimate the bias for different values of $\theta$.
\end{frame}

%-------------------
% Slide 6: Bias estimator non-vectorized algorithm
%-------------------
\begin{frame}[fragile]
\begin{algorithm}[H]
\caption{Monte Carlo Bias Estimation for $\hat{\theta} = \max(U_1, \ldots, U_m)$}
\begin{algorithmic}[1]
  \State \textbf{Input:} Number of replications $n = 1000$, number of samples $m = 10$, number of parameter values $J = 100$
  \State Initialize empty arrays: \texttt{bias[1:J]}, \texttt{theta[1:J]}
  \For{$j = 1$ to $J$}
    \State Set $\theta_j = 3 + 2 \times \dfrac{j}{J}$
    \State Initialize empty list \texttt{samples}
    \For{$i = 1$ to $n$}
      \State Generate $U_{i1}, \ldots, U_{im} \sim \text{Uniform}(0,\, \theta_j)$
      \State Compute $\hat{\theta}_i = \max(U_{i1}, \ldots, U_{im})$
      \State Append $\hat{\theta}_i$ to \texttt{samples}
    \EndFor
    \State Compute $\texttt{mean\_max}_j = \dfrac{1}{n} \sum_{i=1}^n \texttt{samples}_i$
    \State Compute $\texttt{bias}_j = \texttt{mean\_max}_j - \theta_j$
  \EndFor
  \State \textbf{Output:} Arrays \texttt{bias[1:J]} and \texttt{theta[1:J]}
\end{algorithmic}
\end{algorithm}
\end{frame}

%-------------------
% Slide 7: Bias estimator semi-vectorized algorithm
%-------------------
\begin{frame}[fragile]
\begin{algorithm}[H]
\caption{Semi-Vectorized Monte Carlo Bias Estimation for $\hat{\theta} = \max(U_1, \ldots, U_m)$}
\begin{algorithmic}[1]
  \State \textbf{Input:} Number of replications $n = 1000$, number of samples $m = 10$, number of parameter values $J = 100$
  \State Initialize arrays: \texttt{bias[1:J]}, \texttt{theta[1:J]}
  \For{$j = 1$ to $J$}
    \State Set $\theta_j = 3 + 2 \times \dfrac{j}{J}$
    \State Generate matrix $U \sim \text{Uniform}(0,\, \theta_j)$ of size $(n, m)$
    \State Compute vector $\hat{\boldsymbol{\theta}} = \max(U, \text{axis}=1)$
    \State Compute $\texttt{mean\_max}_j = \dfrac{1}{n} \sum_{i=1}^{n} \hat{\theta}_i$
    \State Compute $\texttt{bias}_j = \texttt{mean\_max}_j - \theta_j$
  \EndFor
  \State \textbf{Output:} Arrays \texttt{bias[1:J]} and \texttt{theta[1:J]}
\end{algorithmic}
\end{algorithm}
\end{frame}

%-------------------
% Slide 8: Bias estimator plot
%-------------------
\begin{frame}
\begin{center}
\includegraphics[scale=0.50]{Bias-graph.png}
\end{center}
\end{frame}

%-------------------
% Slide 9: Bias estimator remarks
%-------------------
\begin{frame}{Example Remarks}
From the previous graph we know that the bias seems to be  linear on $\theta$ and that $\widehat{bias}_{\theta}(\hat{\theta}) \approx -0.09\theta$. In other words, {\bf on average $\hat{\theta}(X)$ underestimates $\theta$ by a relative error of 9\%} (at least if $3\leq \theta \leq 5$).
\vspace{2mm}

(Actually, it can be proven that $E(\hat{\theta})=\frac{n}{n+1}\theta=\frac{10}{11}\theta$ exactly but we are supposed not to know this)
\vspace{2mm}

You can use this information to improve the estimator, for example we could construct a new estimator $\hat{\theta_1}=\frac{100}{91}\hat{\theta}$ that will have a smaller bias.

\vspace{2mm}

Four our data we have $\hat{\theta}(x)=3.38445752$  and $\hat{\theta_1}(x)=3.719184$.

\vspace{2mm}

Can we guarantee that for our specific data the error in the estimate $\hat{\theta_1}(x)$ is going to be less than the error of $\hat{\theta}(x)$?

\vspace{2mm}

NO, we cannot guarantee that for specific samples $x$ of $X$. Actually this data was generated with $\theta=3.5$
\end{frame}

%-------------------
% Slide 10: Comparison of Monte Carlo Implementation Efficiency
%-------------------
\begin{frame}{Comparison of Monte Carlo Implementation Efficiency}
\begin{table}[h!]
\centering
\renewcommand{\arraystretch}{1}
\begin{tabular}{lcccc}
\toprule
\textbf{Approach} & \textbf{Python Loops} & \textbf{FLOPs (approx.)} & \textbf{Relative Runtime} \\
\midrule
Fully Loop-Based & Double (over $i,j$) & $2.0\times10^6$ & 1× (slowest) \\
Semi-Vectorized & Outer loop only & $2.0\times10^6$ & $\sim$10× faster \\
Fully Vectorized & None & $3.0\times10^6$ & $\sim$50–100× faster \\
\bottomrule
\end{tabular}
\end{table}

\vspace{3mm}
\centering
\textbf{Observation:} Even with similar FLOP counts, full vectorization reduces Python overhead dramatically, yielding large speedups.
\end{frame}

%-------------------
% Slide 11: Homework
%-------------------
\begin{frame}{Homework}
Consider a random variable with exponential distribution with unknown mean $\mu \in [3,5]$. Out of a random sample of 9 independent random numbers from this distribution we are only given the minimum, the median and the maximum, and we should use that information only to estimate $\mu$.

\vspace{1mm}

The following estimator of $\mu$ is proposed:
\begin{equation*}
	\hat{\mu}(X)=\frac{2min(X)+median(X)+max(X)}{4}
\end{equation*}

a) Use Monte Carlo Simulations to estimate the bias of this estimator, for values of $\mu$ on the interval $[3,5]$. Use $N=1000$.

\vspace{1mm}

b) Based on your answer to a) propose a new estimator $\hat{\theta_1}$  of the form $\hat{\theta_1}(X)=\hat{\theta}(X)+C$, with $C\in \mathbb{R}$ such that that $\hat{\theta_1}$ has a smaller bias than $\hat{\theta}$ if $\mu \in [3,5]$.

\vspace{1mm}

\textbf{Remark}: Exponential distributions with mean $\mu$ have rate parameter $\lambda=1/\mu$.
\end{frame}

\end{document} 